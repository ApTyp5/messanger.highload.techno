# Курсовой проект "NotWhatsApp" #

![logo](https://github.com/ApTyp5/messanger.highload.techno/blob/main/notwatsup.jpg)

***

## Планируемая нагрузка

|Метрика|Комментарий|Значение|
|-------|-----------|:--------:|
|Аудитория|[(ayдитория телеграмма 4 года назад)](https://telegram.org/blog/100-million) / 2| 50 млн человек|
|средний request per day | [rpd 100 миллионов пользователей](https://telegram.org/blog/15-billion) - 15 млрд | 7.5 * 10^12 rpd|
|средний request per hour| rpd / 24 | 3 125 * 10^8 rph|
|средний rpm | rph/60 | 5 208 333 334 rpm|
|средний rps |rpm/60| 86 805 556 rps|

Учитывая, что в худшем случае каждому сообщению будет соответствовать запрос полученных сообщений пользователя то: 

1. Отправка сообщений займет _86 805 556 rps_
2. Запрос полученных сообщений _86 805 556 rps_

Возможны локальных пиков нагрузок, под которые следует выделять в 2 раза больше ёмкости сервиса: 

1. 2 * 100 * 10^6 = __173 611 112 rps__ на запись
1. 2 * 100 * 10^6 = __173 611 112 rps__ на чтение

Также полезно узнать количество _уникальных пользователей в пиковые часы_. Будем считать, что их колчичество
в час равно 20%, а именно _50 000 000 * 20% / 100% = 10 млрд чел/час_. Тогда в минуту в среднем 
чатиться начинает _10 * 10^6 / 60 = 166 667 чел/мин_. Пусть в локальный пик к нам может прийти в 2 раза больше,
то есть _166 667 * 2 = **333 334 макс. чел/мин**_, или _333 334 / 60 = **2778 макс. чел/сек**_.

Так как [средняя пользовательская сессия в watsapp](https://www.content-review.com/articles/46006/#:~:text=%D0%92%20WhatsApp%20%D1%81%D1%80%D0%B5%D0%B4%D0%BD%D1%8F%D1%8F%20%D0%B4%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D1%8C%20%D1%81%D0%B5%D1%81%D1%81%D0%B8%D0%B8,%2C%20%D0%B2%20%D0%B4%D0%B5%D0%BA%D0%B0%D0%B1%D1%80%D0%B5%20%E2%80%93%2013%20%D0%BC%D0%B8%D0%BD%D1%83%D1%82.)
длится 12 минут, то максимум в нашем сервисе будет _12 * 333 334 = **2 * 10^6  уник. подключений**_.

***

## Концептуальное описание
В MVP сервиса будет входить только отправка символьных сообщений, которя будет производиться следующим образом: 
1. К к каждому пользователю формируется очередь сообщений;
2. Пользователь может в любой момент получить сообщения, находящиеся в своей очереди;
3. История сообщений хранится на клиентах.

***

## Особенности функционирования
Иллюстрация всего, что будет описано в этом разделе
![Функционирование системы](https://github.com/ApTyp5/messanger.highload.techno/blob/main/alll.jpg)

### Логическая схема БД
![Схема бд](https://github.com/ApTyp5/messanger.highload.techno/blob/main/schem.jpg)

### Физическая особенность хранения очередей
Очереди сообщений хранятся следующим образом:
1. На каждого пользователя выделяется список, ключом которого является id пользователя.
2. В списке хранятся строковые записи.
3. Каждое отдельное сообщение состоит из 3-х элементов списка (id автора, timestamp, text).
4. После считывания очередь удаляется.

### Шардинг / репликация
 * Шардинг будет производиться по полю user_id;
 * Каждого мастера будут страховать 2 слейва.
 * Операции чтения обрабатываются слейвами, записи - мастерами.

### Протоколы
Обмен трафиком между клиентом и сервисом будет происходить через wss, внутри сервиса - ws, а также внутренний 
протокол бд.

### Терминация SSL
Терминация SSL осуществяется на балансировщиках, обмен трафиком внутри сервиса 
осуществляется по незащищённому каналу.

### Балансировка
L4 балансировка осущетсвляется на уровне DNS. DNS-сервера отдают virtual-ip балансировщиков по алгоритму round-robin.

Virtual-ip, который получает пользователь, ведёт его на один из L7 балансировщиков. Последний по алгоритму
round-robin будет проксировать трафик на один из app-серверов.

***

## Выбор технологий
* СУБД ~---~ Redis, так как он умеет работать со списками, а также имеет 
встроенные шардинг и репликацию;
* L7 балансировщик ~---~ nginx, так как он достаточно эффективен и лекго настраиваем;
* ЯП ~---~ golang, так как он имеет встроенную многопоточность, большое сообщество и его легко изучать;
* Фреймворк ~---~ echo, так как он популярный, простой, с хорошей документацией.

***

## Ресурсные затраты
__Критическим ресурсом__ для проектируемого сервиса является cpu.

### Замеры времени вставки одного сообщения
![Замеры времени вставки одного сообщения](https://github.com/ApTyp5/messanger.highload.techno/blob/main/insert-bench.png)

Длина сообщения была равна 35 русских символов c пробелами, что соответствует 
[средней длине сообщения человека в возрасте 25 лет](https://crushhapp.com/blog/k-wrap-it-up-mom).
Округлим до __30 000 rps__.

### Замеры времени получения одной очереди сообщений
![Замеры времени получения одной очереди сообщений](https://github.com/ApTyp5/messanger.highload.techno/blob/main/read-bench.png)

В очереди сообщений при замерах находится 20 писем.
Округлим до __30 000 rps__.


### Эффективность Appliction серверов на golang:
Судя по [бенчмарку](https://github.com/smallnest/go-web-framework-benchmark), прокидывание запросов к СУБД из golang и последующий возврат ответа
даёт в среднем результаты __30 000 rps__.



### Балансировщики нагрузки:
Исходя из [официальных бенчмарков](https://www.nginx.com/blog/nginx-websockets-performance/), 
nginx на 1 ядре спокойно держит __50 000 подключений__, причем во время бенчмарка по этим подключениям
посылались сообщения размером от 10 до 4096 байт с шагом от 0.1 до 10 секунд, что досаточно точно 
имитирует поведение пользователя мессенджера.

***

## Требуемое оборудование
### Требуемое количество ядер 
|Категория сервера|Вычисления|Количество ядер|
|---------|----------|:---------------:|
|Мастер-ноды|(86 805 556 rps) / (30 * 10^3 rps)|2 894|
|Слейв-ноды|<количество ядер на master-node> * 2|5 788|
|App-серверы|(400 * 10^6 rps) / (30 * 10^3 rps)|5 788|
|Balancers(подключения)|(2 * 10^6 одновременных подключений) / (50 * 10^3 подключений на ядро)| 40 |

На терминацию ssl от _2778 макс. чел/сек_ L7 балансировщикам понадобится, исходя из 
[официальных берчмарков](https://www.nginx.com/blog/testing-the-performance-of-nginx-and-nginx-plus-web-servers/), ещё дополнительно 8 ядер.

Так как DNS многократно кэшируется, то высоконагруженной подсистему DNS назвать нельзя. В нашем
случае DNS сервера будут обеспечивать мониторинг доступности балансировщиков (подробнее 
в обеспечении доступности). Однако для этого
также не нужно много ресурсов, поэтому для этих серверов подойдут 4-хядерные
8-гигабитные машины.

### Расчет потребного оборудования
![Расчет потребного оборудования](https://github.com/ApTyp5/messanger.highload.techno/blob/main/counting.png)

Для серверов будут использованы стандартные 64-х ядерные сервера (24 для балансировщиков), содержащие
либо 32, либо 64 гигабайта памяти. Также будет учтен небольшой запас на случай 
непредвиденных обстоятельств.

|Категория сервера|Комментарий|Количество серверов|
|---------|----------|:---------------:|
|Мастер-ноды|6 667 / 64| 105| 
|Слейв-ноды|105 * 2|210| 
|App-серверы|13 334 / 64| 209| 
|Balancers L7| 3 (2 доп. для отказоустойчивости) | 3 |
|DNS Balancres| 3 (2 доп. для отказоустойчивости) | 3 |

### Итог оснащения сервиса:
|Категория сервера|CPU(cores)|RAM(GB)| Количество|
|---------|----------|:---------------:|:-:|
|Мастер-ноды|64|64| 105|
|Слейв-ноды|64|64| 210|
|App-серверы|64|32| 209|
|Balancers|24|32| 3|
|DNS|4|8|3|
|Запасные|64|64|5|

Запасные сервера пригодятся для быстрого реагирования на непредвиденные 
ситуации.



***

## Инфраструктура
Так как большая часть населения России проживает в европейской части, то 
все сервера будут располагаться в Московской области. Причем каждый из серверов
тройки master-slave-slave должны располагаться на 3-х разных хостингах. Также балансировщики,
application-сервера должны быть равномерно распределены по 3-м хостингам.

***

![Отказоустойчивость](https://github.com/ApTyp5/messanger.highload.techno/blob/main/fixiki.jpg)

Так как сервера распределены по 3-м различным хостингам, то неполадки в одном из 
хостингов не завалят сервис - он останется доступным.

При неполадках в конкретных серверах сервис останется доступным:
1. Если application-сервер вышел из строя, то балансировщики перестанут перенаправлять к нему
запросы - сервис останется доступным.
2. Если балансировщик выйдет из строя, то это обнаружится при heart-beat-проверке (которая 
расположена на DNS-серверах), что повлечет удаление их адресных записей из DNS-таблиц - сервис останется доступным.
3. Если выйдет из строя DNS сервер, то пользователь после неудачного резолвинга адреса 
будет направлен на второй DNS сервер - сервис останется доступным.
3. Если master выйдет из строя, его заменит slave, кластер сменит конфигурацию, после чего 
запросы на запись будут приходить вновь объявленному master - сервис останется доступным.
4. Если slave выйдет из строя, то есть ещё один slave, который начнет принимать все запросы 
на чтание - сервис останется доступным.

При всех этих раскладах у людей будет время на устранение недостатков сервиса и 
если они им правильно воспользоваться, то сервис останется доступным.










